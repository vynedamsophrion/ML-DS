{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb42ca37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import torch\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0875d5c",
   "metadata": {},
   "source": [
    "# Sentiment Analysis with DistilBERT Fine-tuning\n",
    "\n",
    "This notebook demonstrates how to fine-tune a DistilBERT model for sentiment analysis using the IMDB dataset. We'll go through the following steps:\n",
    "\n",
    "1. Setting up the required packages and environment\n",
    "2. Loading and preprocessing the IMDB dataset\n",
    "3. Initializing the DistilBERT model and tokenizer\n",
    "4. Training the model\n",
    "5. Evaluating the model's performance\n",
    "6. Testing the model with custom examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4749e3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# Set device (GPU if available, else CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load IMDB dataset\n",
    "dataset = load_dataset(\"imdb\")\n",
    "print(\"Dataset loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065a630f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# Initialize tokenizer and model\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(model_name)\n",
    "model = DistilBertForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "model = model.to(device)\n",
    "\n",
    "print(\"Model and tokenizer initialized successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32ce357",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the dataset\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "print(\"Dataset tokenized successfully.\")\n",
    "\n",
    "# Convert to PyTorch format\n",
    "tokenized_datasets = tokenized_datasets.remove_columns([\"text\"])\n",
    "tokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")\n",
    "tokenized_datasets.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d43e11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 8\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 2\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create data loaders\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    tokenized_datasets[\"train\"], \n",
    "    shuffle=True, \n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "eval_dataloader = DataLoader(\n",
    "    tokenized_datasets[\"test\"], \n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "print(f\"Created data loaders with batch size: {batch_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e68ffd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "137e66bd254e4a5bb872eb78dab12b98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cef8912b57bd4aa1a373005dcef8b823",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Training setup\n",
    "from torch.optim import AdamW\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "num_epochs = 3\n",
    "\n",
    "print(\"Starting training...\")\n",
    "print(f\"Number of epochs: {num_epochs}\")\n",
    "print(f\"Learning rate: 5e-5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b56582",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    progress_bar = tqdm(train_dataloader, desc=f\"Epoch {epoch + 1}\")\n",
    "    \n",
    "    for batch in progress_bar:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        progress_bar.set_postfix({\"loss\": total_loss / len(train_dataloader)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90486565",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "model.eval()\n",
    "total_correct = 0\n",
    "total_samples = 0\n",
    "\n",
    "print(\"\\nEvaluating the model...\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(eval_dataloader):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "        total_correct += (predictions == batch[\"labels\"]).sum().item()\n",
    "        total_samples += len(batch[\"labels\"])\n",
    "\n",
    "accuracy = total_correct / total_samples\n",
    "print(f\"\\nTest Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "471d0e82",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install \"accelerate>=0.26.0\" \"transformers[torch]\" --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "766eadb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install \"transformers[torch]\" --quiet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cd0f0562",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install \"accelerate>=0.26.0\" --quiet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b4fd5564",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(output_dir: Optional[str] = None, overwrite_output_dir: bool = False, do_train: bool = False, do_eval: bool = False, do_predict: bool = False, eval_strategy: Union[transformers.trainer_utils.IntervalStrategy, str] = 'no', prediction_loss_only: bool = False, per_device_train_batch_size: int = 8, per_device_eval_batch_size: int = 8, per_gpu_train_batch_size: Optional[int] = None, per_gpu_eval_batch_size: Optional[int] = None, gradient_accumulation_steps: int = 1, eval_accumulation_steps: Optional[int] = None, eval_delay: float = 0, torch_empty_cache_steps: Optional[int] = None, learning_rate: float = 5e-05, weight_decay: float = 0.0, adam_beta1: float = 0.9, adam_beta2: float = 0.999, adam_epsilon: float = 1e-08, max_grad_norm: float = 1.0, num_train_epochs: float = 3.0, max_steps: int = -1, lr_scheduler_type: Union[transformers.trainer_utils.SchedulerType, str] = 'linear', lr_scheduler_kwargs: Union[dict[str, Any], str] = <factory>, warmup_ratio: float = 0.0, warmup_steps: int = 0, log_level: str = 'passive', log_level_replica: str = 'warning', log_on_each_node: bool = True, logging_dir: Optional[str] = None, logging_strategy: Union[transformers.trainer_utils.IntervalStrategy, str] = 'steps', logging_first_step: bool = False, logging_steps: float = 500, logging_nan_inf_filter: bool = True, save_strategy: Union[transformers.trainer_utils.SaveStrategy, str] = 'steps', save_steps: float = 500, save_total_limit: Optional[int] = None, save_safetensors: bool = True, save_on_each_node: bool = False, save_only_model: bool = False, restore_callback_states_from_checkpoint: bool = False, no_cuda: bool = False, use_cpu: bool = False, use_mps_device: bool = False, seed: int = 42, data_seed: Optional[int] = None, jit_mode_eval: bool = False, bf16: bool = False, fp16: bool = False, fp16_opt_level: str = 'O1', half_precision_backend: str = 'auto', bf16_full_eval: bool = False, fp16_full_eval: bool = False, tf32: Optional[bool] = None, local_rank: int = -1, ddp_backend: Optional[str] = None, tpu_num_cores: Optional[int] = None, tpu_metrics_debug: bool = False, debug: Union[str, list[transformers.debug_utils.DebugOption]] = '', dataloader_drop_last: bool = False, eval_steps: Optional[float] = None, dataloader_num_workers: int = 0, dataloader_prefetch_factor: Optional[int] = None, past_index: int = -1, run_name: Optional[str] = None, disable_tqdm: Optional[bool] = None, remove_unused_columns: bool = True, label_names: Optional[list[str]] = None, load_best_model_at_end: bool = False, metric_for_best_model: Optional[str] = None, greater_is_better: Optional[bool] = None, ignore_data_skip: bool = False, fsdp: Union[list[transformers.trainer_utils.FSDPOption], str, NoneType] = None, fsdp_min_num_params: int = 0, fsdp_config: Union[dict[str, Any], str, NoneType] = None, fsdp_transformer_layer_cls_to_wrap: Optional[str] = None, accelerator_config: Union[dict, str, NoneType] = None, parallelism_config: Optional[Any] = None, deepspeed: Union[dict, str, NoneType] = None, label_smoothing_factor: float = 0.0, optim: Union[transformers.training_args.OptimizerNames, str] = 'adamw_torch', optim_args: Optional[str] = None, adafactor: bool = False, group_by_length: bool = False, length_column_name: str = 'length', report_to: Union[NoneType, str, list[str]] = None, project: str = 'huggingface', trackio_space_id: Optional[str] = 'trackio', ddp_find_unused_parameters: Optional[bool] = None, ddp_bucket_cap_mb: Optional[int] = None, ddp_broadcast_buffers: Optional[bool] = None, dataloader_pin_memory: bool = True, dataloader_persistent_workers: bool = False, skip_memory_metrics: bool = True, use_legacy_prediction_loop: bool = False, push_to_hub: bool = False, resume_from_checkpoint: Optional[str] = None, hub_model_id: Optional[str] = None, hub_strategy: Union[transformers.trainer_utils.HubStrategy, str] = 'every_save', hub_token: Optional[str] = None, hub_private_repo: Optional[bool] = None, hub_always_push: bool = False, hub_revision: Optional[str] = None, gradient_checkpointing: bool = False, gradient_checkpointing_kwargs: Union[dict[str, Any], str, NoneType] = None, include_inputs_for_metrics: bool = False, include_for_metrics: list[str] = <factory>, eval_do_concat_batches: bool = True, fp16_backend: str = 'auto', push_to_hub_model_id: Optional[str] = None, push_to_hub_organization: Optional[str] = None, push_to_hub_token: Optional[str] = None, mp_parameters: str = '', auto_find_batch_size: bool = False, full_determinism: bool = False, torchdynamo: Optional[str] = None, ray_scope: Optional[str] = 'last', ddp_timeout: int = 1800, torch_compile: bool = False, torch_compile_backend: Optional[str] = None, torch_compile_mode: Optional[str] = None, include_tokens_per_second: bool = False, include_num_input_tokens_seen: Union[str, bool] = False, neftune_noise_alpha: Optional[float] = None, optim_target_modules: Union[NoneType, str, list[str]] = None, batch_eval_metrics: bool = False, eval_on_start: bool = False, use_liger_kernel: bool = False, liger_kernel_config: Optional[dict[str, bool]] = None, eval_use_gather_object: bool = False, average_tokens_across_devices: bool = True) -> None\n",
      "\n",
      "    TrainingArguments is the subset of the arguments we use in our example scripts **which relate to the training loop\n",
      "    itself**.\n",
      "\n",
      "    Using [`HfArgumentParser`] we can turn this class into\n",
      "    [argparse](https://docs.python.org/3/library/argparse#module-argparse) arguments that can be specified on the\n",
      "    command line.\n",
      "\n",
      "    Parameters:\n",
      "        output_dir (`str`, *optional*, defaults to `\"trainer_output\"`):\n",
      "            The output directory where the model predictions and checkpoints will be written.\n",
      "        overwrite_output_dir (`bool`, *optional*, defaults to `False`):\n",
      "            If `True`, overwrite the content of the output directory. Use this to continue training if `output_dir`\n",
      "            points to a checkpoint directory.\n",
      "        do_train (`bool`, *optional*, defaults to `False`):\n",
      "            Whether to run training or not. This argument is not directly used by [`Trainer`], it's intended to be used\n",
      "            by your training/evaluation scripts instead. See the [example\n",
      "   \n"
     ]
    }
   ],
   "source": [
    "# Debug: show TrainingArguments signature\n",
    "import inspect\n",
    "print(inspect.signature(TrainingArguments))\n",
    "print(TrainingArguments.__doc__[:1000])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3ca81505",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import inspect\n",
    "params = inspect.signature(TrainingArguments).parameters\n",
    "print('evaluation_strategy' in params)\n",
    "print('logging_steps' in params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ebbe0671",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Average Loss = 0.6935, Accuracy = 0.50\n",
      "Epoch 2: Average Loss = 0.6316, Accuracy = 1.00\n",
      "Epoch 2: Average Loss = 0.6316, Accuracy = 1.00\n",
      "Epoch 3: Average Loss = 0.6110, Accuracy = 1.00\n",
      "Epoch 3: Average Loss = 0.6110, Accuracy = 1.00\n"
     ]
    }
   ],
   "source": [
    "# Test with custom examples\n",
    "def predict_sentiment(text):\n",
    "    # Tokenize input\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True).to(device)\n",
    "    \n",
    "    # Get prediction\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        predictions = torch.softmax(outputs.logits, dim=-1)\n",
    "        prediction = torch.argmax(predictions, dim=-1)\n",
    "        confidence = predictions[0][prediction[0]].item()\n",
    "    \n",
    "    sentiment = \"Positive\" if prediction.item() == 1 else \"Negative\"\n",
    "    return sentiment, confidence\n",
    "\n",
    "# Test examples\n",
    "test_texts = [\n",
    "    \"This movie was absolutely amazing! I loved every minute of it.\",\n",
    "    \"What a terrible waste of time. I wouldn't recommend this to anyone.\",\n",
    "    \"The film was okay, but nothing special.\"\n",
    "]\n",
    "\n",
    "print(\"Testing model with example reviews:\\n\")\n",
    "for text in test_texts:\n",
    "    sentiment, confidence = predict_sentiment(text)\n",
    "    print(f\"Text: {text}\")\n",
    "    print(f\"Sentiment: {sentiment} (Confidence: {confidence:.4f})\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d95d7307",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I really enjoyed this! â†’ Positive (0.60 confidence)\n",
      "This was disappointing and boring. â†’ Negative (0.51 confidence)\n"
     ]
    }
   ],
   "source": [
    "# ðŸ’¬ Cell 8 â€” Test Predictions\n",
    "test_texts = [\n",
    "    \"I really enjoyed this!\", \n",
    "    \"This was disappointing and boring.\"\n",
    "]\n",
    "\n",
    "inputs = tokenizer(test_texts, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "outputs = model(**inputs)\n",
    "predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "labels = torch.argmax(predictions, dim=1)\n",
    "\n",
    "for i, (text, label) in enumerate(zip(test_texts, labels)):\n",
    "    # show per-sample confidence for the chosen label\n",
    "    confidence = predictions[i][label].item()\n",
    "    print(f\"{text} â†’ {'Positive' if label == 1 else 'Negative'} ({confidence:.2f} confidence)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c3f1fd",
   "metadata": {},
   "source": [
    "# ðŸ“Š Cell 9 â€” Reflection (Markdown)\n",
    "\n",
    "\"\"\"\n",
    "Reflection:\n",
    "Through this project, I learned how to fine-tune a pre-trained transformer model (DistilBERT) for a custom classification task.\n",
    "It was interesting to see how powerful transfer learning can be even with a small dataset.\n",
    "The main challenge was setting up the tokenizer and ensuring correct input shapes for training.\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
